import tensorflow as tf
tf.set_random_seed(666)

t_x = [2, 4, 6, 8]
t_y = [81, 93, 91, 97]

W = tf.Variable(tf.random_uniform([1], 0, 10, dtype=tf.float64, seed=0))
b = tf.Variable(tf.random_uniform([1], 0, 100, dtype=tf.float64, seed=0))

# Our hypothesis XW+b
hypo = t_x * W + b

# cost/loss function
rmse = tf.sqrt(tf.reduce_mean(tf.square(hypo - t_y)))

# Minimize
optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.1)
train = optimizer.minimize(rmse)

# Launch the graph in a session.
sess = tf.Session()
# Initializes global variables in the graph.
sess.run(tf.global_variables_initializer())

# Fit the line
for step in range(4001):
    sess.run(train)
    if step % 100 == 0:
        print(step, sess.run(rmse), sess.run(W), sess.run(b))

sess.close()

#0 69.81924738183868 [1.71474503] [12.17876819]
#100 25.19771408395535 [12.57411991] [17.68771571]
#200 23.598954913580982 [11.9134219] [21.63120948]
#300 22.00347970397614 [11.2532835] [25.57063312]
#400 20.412049027920904 [10.59398192] [29.50506298]
#500 18.825673868553224 [9.9357278] [33.43324206]
#600 17.24572564677031 [9.27880765] [37.35346058]
#700 15.67410823171893 [8.62362061] [41.2633366]
#800 14.113535299815686 [7.97073826] [45.15945928]
#900 12.567993023504059 [7.32100544] [49.03678687]
#1000 11.043540413876539 [6.67571765] [52.88758841]
#1100 9.5497446330689 [6.03694679] [56.69949974]
#1200 8.102331446590911 [5.40816569] [60.45179643]
#1300 6.728096929982016 [4.79548333] [64.10802272]
#1400 5.473304900568014 [4.2100542] [67.60161339]
#1500 4.413069896946557 [3.67206048] [70.81212986]
#1600 3.6401105976431225 [3.21320073] [73.55040861]
#1700 3.193243640466592 [2.86527166] [75.62670048]
#1800 2.9932567574202436 [2.63333552] [77.01079617]
#1900 2.918739909909538 [2.49209242] [77.85367458]
#2000 2.8933511709817066 [2.40973399] [78.34515446]
#2100 2.8849934523438683 [2.36249859] [78.62703514]
#2200 2.8822745671119803 [2.33556066] [78.78778918]
#2300 2.8813935282767558 [2.32022692] [78.87929442]
#2400 2.881108396300404 [2.31150388] [78.93134979]
#2500 2.88101615669473 [2.3065425] [78.96095713]
#2600 2.8809863213611475 [2.30372082] [78.97779571]
#2700 2.880976671399718 [2.30211608] [78.98737212]
#2800 2.880973550253007 [2.30120344] [78.99281836]
#2900 2.8809725407657996 [2.30068441] [78.99591571]
#3000 2.880972214263056 [2.30038923] [78.99767721]
#3100 2.880972108660944 [2.30022136] [78.998679]
#3200 2.880972074505624 [2.30012589] [78.99924873]
#3300 2.880972063458626 [2.3000716] [78.99957274]
#3400 2.8809720598856554 [2.30004072] [78.99975701]
#3500 2.880972058730037 [2.30002316] [78.99986181]
#3600 2.8809720583562646 [2.30001317] [78.99992141]
#3700 2.8809720582353755 [2.30000749] [78.9999553]
#3800 2.8809720581962788 [2.30000426] [78.99997458]
#3900 2.88097205818363 [2.30000242] [78.99998554]
#4000 2.880972058179545 [2.30000138] [78.99999178]

